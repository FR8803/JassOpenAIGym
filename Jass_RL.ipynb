{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Jass_RL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZDCRYQsWYmr"
      },
      "source": [
        "#more details on the code can be found here https://www.tensorflow.org/agents/tutorials/1_dqn_tutorial\n",
        "#!!! if this code was to be used by anyone other than the original author, certain lines might have to be tweaked (such as the google drive path for examples)\n",
        "!pip install gym\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install PILLOW\n",
        "!pip install -pyglet\n",
        "!pip install tf-agents\n",
        "!pip install optuna\n",
        "!pip install -q 'xvfbwrapper==0.2.9'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baRJdqD8VQHX"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import gym\n",
        "import os\n",
        "from functools import partial\n",
        "import tempfile\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tf_agents\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import dynamic_step_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.policies import random_tf_policy, epsilon_greedy_policy, policy_saver\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "\n",
        "from gym import envs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NytRe0pTWVXn"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/drive/My Drive/Jass_RL/JassOpenAIGym/\")\n",
        "import gym_Jass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40gcqfuS8h5S"
      },
      "source": [
        "#setup hyperparameters\n",
        "num_iterations = 500000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"} \n",
        "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 25000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 128  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-2  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhPeug01r_wM"
      },
      "source": [
        "#initialize the TPUs\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BwmtkSHyhUk"
      },
      "source": [
        "#definition of the epsilon decay\n",
        "train_step = tf.Variable(0, trainable=False, name='global_step', dtype=tf.int64)\n",
        "def exponentially_decaying_epsilon(train_step, start_epsilon = 1, end_epsilon =0.01, decay = 0.999995, step_number = num_iterations):\n",
        "  #convert tensor to normal integer\n",
        "  train_step = train_step.numpy()\n",
        "  epsilon = max(start_epsilon * decay**train_step, end_epsilon)\n",
        "  return epsilon\n",
        "exponentially_decaying_epsilon(train_step)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYHyUamLCKGG"
      },
      "source": [
        "#loads the gym, set ups the environment, the q-network, the optimizer and the learning agent\n",
        "train_py_env = suite_gym.load(\"Jass-v0\")\n",
        "eval_py_env = suite_gym.load(\"Jass-v0\")\n",
        "\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "#there are 180 input nodes and 9 output nodes, fc_layer_params sets the number of nodes in the hidden layers\n",
        "fc_layer_params = (128, 36)\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "\n",
        "  q_net = q_network.QNetwork(\n",
        "      train_env.observation_spec(),\n",
        "      train_env.action_spec(),\n",
        "      fc_layer_params=fc_layer_params)\n",
        "\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "with strategy.scope():\n",
        "\n",
        "  \n",
        "  global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "  train_step = tf.Variable(0, trainable=False, name='global_step', dtype=tf.int64)\n",
        "  partial_exponentially_decaying_eps = partial(exponentially_decaying_epsilon, train_step)\n",
        "\n",
        "  agent = dqn_agent.DqnAgent(\n",
        "      train_env.time_step_spec(),\n",
        "      train_env.action_spec(),\n",
        "      q_network=q_net,\n",
        "      epsilon_greedy= partial_exponentially_decaying_eps,\n",
        "      optimizer=optimizer,\n",
        "      gamma = 0.99,\n",
        "      td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "      train_step_counter=global_step)\n",
        "\n",
        "  agent.initialize()\n",
        "\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())\n",
        "example_environment = tf_py_environment.TFPyEnvironment(\n",
        "    suite_gym.load('Jass-v0'))\n",
        "\n",
        "time_step = example_environment.reset()\n",
        "\n",
        "random_policy.action(time_step)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8X7F3XsSt8SZ"
      },
      "source": [
        "#summarizes the specs of the Q network that has been setup\n",
        "q_net.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPH_TNggGynI"
      },
      "source": [
        "#computes the average return over a number of episodes (for testing purposes)\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkJ81Fd8sw7y"
      },
      "source": [
        "#testing of step and episode, not relevant for the training of the models\n",
        "env = suite_gym.load('Jass-v0')\n",
        "tf_env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "time_step = tf_env.reset()\n",
        "rewards = []\n",
        "steps = []\n",
        "num_episodes = 5\n",
        "\n",
        "for _ in range(num_episodes):\n",
        "  episode_reward = 0\n",
        "  episode_steps = 0\n",
        "  while not time_step.is_last():\n",
        "    action = tf.random.uniform([1], 0, 2, dtype=tf.int32)\n",
        "    time_step = tf_env.step(action)\n",
        "    episode_steps += 1\n",
        "    episode_reward += time_step.reward.numpy()\n",
        "  rewards.append(episode_reward)\n",
        "  steps.append(episode_steps)\n",
        "  time_step = tf_env.reset()\n",
        "\n",
        "num_steps = np.sum(steps)\n",
        "avg_length = np.mean(steps)\n",
        "avg_reward = np.mean(rewards)\n",
        "\n",
        "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
        "print('avg_length', avg_length, 'avg_reward:', avg_reward)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlkJ5z16cN01"
      },
      "source": [
        "#connects google colab and google cloud, for authentification purposes\n",
        "from google.colab import auth\n",
        "\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OdNinWcRYrt"
      },
      "source": [
        "#setup of the replay buffer\n",
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=replay_buffer_max_length)\n",
        "\n",
        "def collect_step(environment, policy, buffer):\n",
        "  time_step = environment.current_time_step()\n",
        "  action_step = policy.action(time_step)\n",
        "  next_time_step = environment.step(action_step.action)\n",
        "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "\n",
        "  # Add trajectory to the replay buffer\n",
        "  buffer.add_batch(traj)\n",
        "\n",
        "def collect_data(env, policy, buffer, steps):\n",
        "  for _ in range(steps):\n",
        "    collect_step(env, policy, buffer)\n",
        "\n",
        "collect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n",
        "\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, \n",
        "    sample_batch_size=batch_size, \n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "iterator = iter(dataset)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKkfl92jqK3n"
      },
      "source": [
        "#set up of the Google cloud environment, in order for the modells to be saved there\n",
        "project_id = \"gentle-ally-303519\"\n",
        "bucket_name = \"rl_jass\"\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GajBen0qLj2"
      },
      "source": [
        "#implementing checkpointer and policy saver to later load the model\n",
        "checkpoint_dir = \"gs://rl_jass\"\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=global_step\n",
        ")\n",
        "\n",
        "policy_dir = \"gs://rl_jass\"\n",
        "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gujYY4EevG1l"
      },
      "source": [
        "#training of the RL-algorithm\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "loss = []\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    print(exponentially_decaying_epsilon(agent.train_step_counter))\n",
        "    loss.append(train_loss.numpy())\n",
        "    returns.append(avg_return)\n",
        "  if step % 50000 == 0:\n",
        "    train_checkpointer.save(global_step)\n",
        "    np.savez(\"outfile.npz\", loss, returns)\n",
        "    !cp outfile.npz \"/content/drive/My Drive/\"\n",
        "  if step == 500000:\n",
        "    tf_policy_saver.save(policy_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ir5t9Y_JX_s"
      },
      "source": [
        "#loss and average returns are being saved in the outfile.npz, so that they don't get lost after training\n",
        "outfile_npz= np.load(\"outfile.npz\")\n",
        "loss = outfile_npz[\"arr_0\"].tolist()\n",
        "returns = outfile_npz[\"arr_1\"].tolist()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzmhwchxCDJX"
      },
      "source": [
        "#visualization of average returns\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qttQBzHdCVvN"
      },
      "source": [
        "#visualization of the loss\n",
        "iterations = range(0, num_iterations, eval_interval)\n",
        "plt.plot(iterations, loss)\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Iterations')\n",
        "#plt.ylim(top=15000000)\n",
        "plt.axis([0, 100000, 0, 100000])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHwHPHy2v-rn"
      },
      "source": [
        "#loading the modell from Google cloud / initializing the checkpointer from Google Cloud if training has to be continued and then continuing the training process\n",
        "policy = agent.policy\n",
        "\n",
        "global_step = tf.compat.v1.train.get_global_step()\n",
        "\n",
        "policy_checkpointer = common.Checkpointer(ckpt_dir= checkpoint_dir, global_step = global_step, policy=policy, agent = agent)\n",
        "policy_checkpointer.initialize_or_restore()\n",
        "\n",
        "outfile_npz= np.load(\"outfile.npz\")\n",
        "loss = outfile_npz[\"arr_0\"].tolist()\n",
        "returns = outfile_npz[\"arr_1\"].tolist()\n",
        "\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "my_policy = agent.collect_policy\n",
        "\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
        "  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    print(exponentially_decaying_epsilon(agent.train_step_counter))\n",
        "    loss.append(train_loss.numpy())\n",
        "    returns.append(avg_return)\n",
        "  if step % 50000 == 0:\n",
        "    train_checkpointer.save(global_step)\n",
        "    tf_policy_saver.save(policy_dir)\n",
        "    np.savez(\"outfile.npz\", loss, returns)\n",
        "    !cp outfile.npz \"/content/drive/My Drive/\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZkXxjjRoQDC"
      },
      "source": [
        "#evaluating the environment for another 10'000 games and returning certain key statistics\n",
        "eval_py_env = suite_gym.load(\"Jass-v0\")\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "\n",
        "saved_policy = tf.compat.v2.saved_model.load(policy_dir)\n",
        "num_eval_episodes = 10000\n",
        "avg_return = compute_avg_return(eval_env, saved_policy, num_eval_episodes)\n",
        "\n",
        "print(eval_py_env.game_dict)\n",
        "print(avg_return)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbUvR5XaL2hI"
      },
      "source": [
        "#plotting the exponential decay\n",
        "def exponentially_decaying_epsilon_plot(start_epsilon = 1, end_epsilon = 0.05, decay = 0.999995, step_number = num_iterations):\n",
        "  epsilon = start_epsilon\n",
        "  steps = []\n",
        "  step_counter = 0\n",
        "  epsilon_arr = []\n",
        "  for i in range(step_number): \n",
        "    step_counter += 1\n",
        "    steps.append(step_counter)\n",
        "    epsilon = max(epsilon * decay, end_epsilon)\n",
        "    epsilon_arr.append(epsilon)\n",
        "  \n",
        "  plt.plot(steps, epsilon_arr)\n",
        "exponentially_decaying_epsilon_plot()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSfBph_VqhHO"
      },
      "source": [
        "#plotting a linear decay\n",
        "def linearly_decaying_epsilon(start_epsilon = 1, end_epsilon = 0.01, step_number = num_iterations, plot = False, train = True):\n",
        "  #until when does the epsilon decay (number of steps to go from start_epsilon to end_epsilon) in this case at 85% of the learning process\n",
        "  num_learn_iterations = num_iterations * 0.85\n",
        "  epsilon = start_epsilon\n",
        "  steps = []\n",
        "  step_counter = 0\n",
        "  epsilon_arr = []\n",
        "  for i in range(num_iterations):\n",
        "    if train == True:\n",
        "      step_counter = agent.train_step_counter.numpy()\n",
        "    else: \n",
        "      step_counter += 1\n",
        "    steps.append(step_counter)\n",
        "    epsilon = max(end_epsilon, epsilon - ((start_epsilon-end_epsilon) / num_learn_iterations))\n",
        "    epsilon_arr.append(epsilon)\n",
        "  if plot == True:\n",
        "    plt.plot(steps, epsilon_arr)\n",
        "linearly_decaying_epsilon(plot = True, train = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}